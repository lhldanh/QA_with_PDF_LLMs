{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\QA_PDF_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loader = PyPDFLoader\n",
    "file_path = 'YOLOv10_Tutorials.pdf'\n",
    "\n",
    "loader = Loader(file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                               chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sub-docs 33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='AI VIET NAM – AI COURSE 2024\\nTutorial: Phát hiện đối tượng trong ảnh với\\nYOLOv10\\nDinh-Thang Duong, Nguyen-Thuan Duong, Minh-Duc Bui và\\nQuang-Vinh Dinh\\nNgày 20 tháng 6 năm 2024\\nI. Giới thiệu\\nObject Detection (Tạm dịch: Phát hiện đối tượng) là một bài toán cổ điển thuộc lĩnh vực\\nComputer Vision. Mục tiêu của bài toán này là tự động xác định vị trí của các đối tượng trong\\nmột tấm ảnh. Tính tới thời điểm hiện tại, đã có rất nhiều phương pháp được phát triển nhằm\\ngiải quyết hiệu quả bài toán này. Trong đó, các phương pháp thuộc họ YOLO (You Only Look\\nOnce) thu hút được sự chú ý rất lớn từ cộng đồng nghiên cứu bởi độ chính xác và tốc độ thực\\nthi mà loại mô hình này mang lại.\\nHình 1: Logo của mô hình YOLO. Ảnh: link.\\nThời gian vừa qua, Ao Wang và các cộng sự tại Đại học Thanh Hoa (Tsinghua University)\\nđã đề xuất mô hình YOLOv10 trong bài báo YOLOv10: Real-Time End-to-End Object\\nDetection [10]. Với những cải tiến mới, mô hình đã đạt được hiệu suất vượt trội hơn so với các', metadata={'source': 'YOLOv10_Tutorials.pdf', 'page': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print('Number of sub-docs', len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\QA_PDF_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_db = Chroma.from_documents(documents=docs,\n",
    "                               embedding=embedding)\n",
    "\n",
    "retriever = vec_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\n1! mkdir datasets\\n2! unzip -q \"/ content / PlantDocv4 . zip\" -d \"/ content / datasets / PlantDocv4\\n/\"\\n3!rm / content / PlantDocv4 .zip\\nQuan sát thư mục giải nén, có thể thấy bộ dữ liệu này đã được gán nhãn và đưa vào format\\ncấu trúc dữ liệu training theo yêu cầu của YOLO. Vì vậy, chúng ta sẽ không cần thực hiện\\nbước chuẩn bị dữ liệu ở bài này.\\n2.Cài đặt và import các thư viện cần thiết: Tương tự như phần trước, các bạn chạy\\ncác đoạn code sau để cài đặt các gói thư viện để sử dụng được YOLOv10:\\n1!git clone https :// github .com/THU -MIG/ yolov10 .git\\n2%cd yolov10\\n3!pip install -q -r requirements .txt\\n4!pip install -e .\\n3.Khởi tạo mô hình YOLOv10: Chúng ta sẽ khởi tạo mộ hình YOLOv10 với phiên\\nbảnnano (n) từ trọng số đã được huấn luyện trên bộ dữ liệu COCO. Để tải trọng số\\nyolov10n.pt, các bạn chạy đoạn code sau:\\n1! wget https :// github .com/THU -MIG/ yolov10 / releases / download /v1 .1/\\nyolov10n .pt', metadata={'page': 15, 'source': 'YOLOv10_Tutorials.pdf'}),\n",
       " Document(page_content='AI VIETNAM (AIO2024) aivietnam.edu.vn\\nIV. Cài đặt chương trình và đánh giá\\nTrong phần này, nhóm sẽ trình bày cách cài đặt, sử dụng và huấn luyện YOLOv10 trên bộ dữ\\nliệu mới. Đồng thời, nhóm cũng thực hiện một thực nghiệm nhỏ nhằm so sánh hiệu suất của\\nYOLOv10 so với hai phiên bản gần nhất là YOLOv8 và YOLOv9. Môi trường lập trình nhóm\\nsử dụng là Google Colab.\\nIV.I. Cài đặt và sử dụng pre-trained model\\nMột cách nhanh chóng để sử dụng được YOLOv10 đó là sử dụng pre-trained model (mô hình đã\\nđược huấn luyện sẵn trên bộ dữ liệu COCO - một bộ dữ liệu rất lớn). Để sử dụng pre-trained\\nmodel, các bạn làm như sau:\\n1.Cài đặt các thư viện cần thiết: Tải về mã nguồn của YOLOv10 và cài đặt các thư\\nviện trong file requirements.txt bằng các chạy đoạn code sau:\\n1!git clone https :// github .com/THU -MIG/ yolov10 .git\\n2%cd yolov10\\n3!pip install -q -r requirements .txt\\n4!pip install -e .\\n2.Tải trọng số của pre-trained models: Để sử dụng được pre-trained models, chúng ta', metadata={'page': 12, 'source': 'YOLOv10_Tutorials.pdf'}),\n",
       " Document(page_content='1! wget https :// github .com/THU -MIG/ yolov10 / releases / download /v1 .1/\\nyolov10n .pt\\nSau đó, để khởi tạo mô hình từ trọng số đã tải về, các bạn chạy đoạn code sau:\\n1from ultralytics import YOLOv10\\n2\\n3model = YOLOv10 (\" yolov10n .pt\")\\n4.Huấnluyệnmôhình: ChúngtatiếnhànhhuấnluyệnYOLOv10trênbộdữliệuPlantDoc\\nvới 100 epochs và kích thước ảnh là 640. Các bạn chạy đoạn code sau:\\n1model . train ( data =\"../ datasets / PlantDocv4 / data . yaml \",\\n2 epochs =100 ,\\n3 imgsz =640)\\n16', metadata={'page': 15, 'source': 'YOLOv10_Tutorials.pdf'}),\n",
       " Document(page_content='2.Tải trọng số của pre-trained models: Để sử dụng được pre-trained models, chúng ta\\ncần tải về file trọng số (file .pt). Các bạn chạy đoạn code sau để tải về file trọng số phiên\\nbản YOLOv10n:\\n1! wget https :// github .com/THU -MIG/ yolov10 / releases / download /v1 .1/\\nyolov10n .pt\\n3.Khởi tạo mô hình: Để khởi tạo mô hình với trọng số vừa tải về, các bạn chạy đoạn code\\nsau:\\n1from ultralytics import YOLOv10\\n2\\n3model = YOLOv10 (\" yolov10n .pt\")\\n4.Tải ảnh cần dự đoán: Chúng ta sẽ test mô hình trên một ảnh bất kì. Các bạn có thể tự\\nchọn ảnh của riêng mình hoặc sử dụng ảnh tại đây. Các bạn có thể chạy đoạn code sau để\\ntải ảnh này vào colab tự động:\\n1! gdown \"1 tr9PSRRdlC2pNir7jsYugpSMG -7 v32VJ \" -O \"./ images /\"\\n13', metadata={'page': 12, 'source': 'YOLOv10_Tutorials.pdf'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = retriever.invoke(\"What is YOLO?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = nf4_config,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=model_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x00000218279B0B80>)\n",
       "           | RunnableLambda(format_docs),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n",
       "| HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x00000218CDE5D970>)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\QA_PDF_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "d:\\anaconda3\\envs\\QA_PDF_env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object detection là một bài toán trong lĩnh vực Computer Vision, nó được xác định bằng việc tự động xác định vị trí và nhận diện tên (class) của các vật thể trong một bức ảnh. Có ba hướng tiếp cận chính để giải quyết bài toán này: One-stage Object Detection, Two-stage Object Detection và End-to-end Object Detection.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"Object detection là bài toán gì?\"\n",
    "output = rag_chain.invoke(user_question)\n",
    "answer = output.split(\"Answer: \")[1].strip()\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA_PDF_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
